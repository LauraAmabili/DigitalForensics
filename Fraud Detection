# perso prima lezione fraud :( non ho committato
## 18/03/2022
### Fraud


supervised look inside the huge amount of data 
unserivised: how they behave. 
extract features from data 
such kind of systems are not enough , complement the system with supervised.
learn the diffrence between the known 

un vs sup .> you want to apply unservised every time you ahve a data set but not labeled
supervaside data with label legitimed and unlegitimed -> predict, detect and estimate some features

limitation of supervised learning techiques: is it not enough 

start knownladge from the domain 

antifraud detection system I have to consider the information I have available


start with unsupervised learning that extract novel patterns, once you have the investigation of the anaylist you start with data from the past, find pattern and then supervised 

Fraud managment cycle: 
Fraud detection -> detect it 
Fraud Investigation: every thing flagged as fraud must be investigate by humane EVERY TIME, in order to check if it is a really fraud or not, it's important, allows to understand the knowledgne to use for automted systems
Fraud Confirmation: not, ok this is not fraudolent, change the label, retrain your system, unless put and coferm the label and go on 
Fraud Prevention : now we have to prevent these transaction

feedback loop-> you have to automatize all these process and we will automatize this process-> you have to update all your models and systems (the supervised and unsupervised to mathc the new pattern) 


Fraud analytics process model 



Fraud 3

Machine learning for Fraud Detection 
pefrorm detection 

Real data is typically dirtu, full of inconsistencies, incompleteness and duplication, from messy data you get messy analytical models
so we have to apply data filtering mechanisms
first step of data anlyting preprocessing part
data preprocessing step -> 80% of your time
lose a lot of time here, data are pain if you have ever worked with a real dataset

Data-filtering mechanisms must be applied to clean up and reduce the data.
Even the slightest mistake can make the data totally unusable and the results invalid.
 
-transactional data: structured and detailed information captyring the key characteristics of a customer transaction
Summarized over longer time horizons by aggregating it -> they are meaningful when interpreted individually and furthermore their interaction is useful for fraud detectio  and anti money laundering
-types of data elements
continuous data: defined on an interval ( limited and unlimited) 
categorial:all the set of data defined on a category  
  -nominal: no meaniningful ordering between a limited set of data
  -ordinal: meaningful ordering
  -binary: two values 

-sampling: take a subset of historical data to build an analytical model 
we not analyze directly the full data set because the key requirement for a good sample is to be representative fot the future entities 


 
Take a subset of historical data to build an analytical model.
With the availability of high-performance computing
why not analyze directly the full data set?
 
  sampling procedure: problem of bias, biased on your decision/selection
  taking a sample is biased by our decision
  a good sample should not be biased 
  model and select sample that target the avaerage period and behavior, i want an average picture
  but this is not an easy task 

what can I do ? 
-build separate models for homogenous time frames or different frames-> complex and demanding solution
-sampling observetions over a period covering a full business cycle and build a single model

Stratified sampling: a sample is taken according to predefined strata-> stratifying according to the target fraud indicator-> sample will contain exactly the same percentages of (non-) fraudulent transactions as in the original data
-> stratification applied on predictor variables-> resemble the real product transaction distribution 



Visual data exploration is important, gives you an initial insights into the data, you can use plots
Then inspect some basic statistical measurements -> calculate them separately for each of the target classes to see whether there are any interesting patterns present 

-basic descriptive statistics
descriptive statistics provide basic insight for the data, they should be assessed together in support and completion of each other
-MEAN AND MEDIAN
-VARIATION/STANDARD VARIATION 
-PERCENTILE VALUES
-MODE

-specific descritive statistics
-SYMMETRY OR ASYMMETRY OF A DISTRIBUTION -> harder to interpret: limits their pratical use, sometimes it is easier to assess these aspects by inspecting visual plots of the distribution of the involved variables

-missing values
Some analytical techniques (e.g., decision trees) can deal directly with missing values. Other techniques need some additional preprocessing.
 a. 
 1. Replace the missing values with a known value
 2. Delete observations or variables with lots of missing values: assumes information has no meaningful interpretation 
 3. Keep missing values since they can be meaningful and may have relation with fraud and nees to be considered as a separate category 
b. Statistically test whether missing information is related to the target variable or not.
1. if yes, then we can adopt the keep strategy and make a special category for it
2. if not, one can depending on the number of observations available- delete or replace

-outliers 
extreme observations that are very dissimilar to the rest of the population, can be valid or invalid , unvariate(one dimension) or multivariate(multiple dimensions)
  UNIVARIATE OUTLIER DETECTION AND TREATMENS
  .minimum and maximum values for each of the data elements 
  .graphical tools: histograms, box plot, z-scores
  compute the distribution, represent it on an istogram 
  if inside the range, plot all the data on this line, you compute statistics on your data, then you plot the rane first/last quartile-> how much of your data is inside this area
  ho wmany stanrdard deviation an observation lies away from the mean-> you check for each single instance how much differs from the media in terms of standard deviation 

  MULTIVARIATE OUTLIER DETECTION AND TREATMENS
  -distance between different vectors
  .fitting regression lines and inspecting the observations with large errors
  .clustering or calculating the Mahalanobis distance


-variouys schemes exist to deal with outliers 
  for invalid observations one could treat the putlier as a missing value
  for valid obs impose both a lower and upper limit on a variable and any values below/above are brought back to these limits 

Construct a set of rules formulated based on expert knowledge 
find relation you know must be present in your dataset
constraint that apply 

take explicit precautions

theser kind of inconsistences can by find by experience
## preprocessing
extremly careful to treat these values, you have to ducment by addign a feature that specify because 
it is missing a valueif you start a preprocessing in the wrong way you cannot be more able to detect the anomaly 

another problem: represented by this image : INSERT image
compare two value of your dataset -> the distance ( eucledian, manhattan..)
the problem of this computation is that it is not correct ? 
The scale are differt!!! doesn't make any sense 
u have to standardize your features !!
standardize data-> scaling variables to a similar range
-mix/man standardize
-z-score 
-decimal scaling 

## Categorization 
Additional information about the domain, sometimes you want to add to the dataset some knoldegne of the domain you are analyzing 
some times you have also continuous variables, and uoi have to study the slot and defining when there is a change (increase, dcrease, ioncrease) 
depending on the data you may want to extract different informatio that can be useful -> but you are adding complexity, feeatures to te original one that must be treated

## Variable selection 
problem automatically solved by random forest alghoritms, they learn the feature important to you, but in another case you don't want to spend complexity and add features to your model 
this is a problem cause sometimes the dataset has a huge amount of features, and in the end only few of them are really helpful-> 10/15 variables

depending on the type of the variable and the type of target variable  you can select a different statistic test that measure the correlation between the target and the variable

additional metodology to analyze the correlation


sometimes you cannot use all the feature 
the motivation may contain personal information 
feature seletion independent from your decision 

## PCA 
group of methods to automatically reduce the dimensionality of your dataset and at the same time finds a set of feature not correlated with each other
you are able to reduce, select feature and the feature are not correlated

reduce dim by forming new value that are a linear composition of the original ones (main component) 
at maximum the same number of the original ones 
only a small portion of your variable contains the information you need 

find two new features that better describe the distribution of the data in the space
 PC1 will describe the set of data in a way that you can continue the analysis

 ## correlation and stability 
 assure that your model will be stable


trade-off: PCA if your are not interested in interpretaibility 
if you need to have a score easy to investigate/interpret, you have to avoid PCA


# DESCRIPTIVE ANALYTICS FOR FRAUD DETECTION
unservised learning techniques
unsupervides aims at finding anomalos behavior that deviates from the norm of the dataset
you have to define the norm ( average behiuour of dataset of customer)

unsupervised , you can identify from know cyber attack 

techiques relevant when you have NO labels

define your norm, define the AVERAGE behavior of your dataset 

procedure similar for outliers in preprocessing, you can canalyze them in one or multi dimension 


knowledge to verify what you have found

statistical outlier detection 
-> z-score
-fit a distribution 
these two are the same saw before
-> break point analysis
instra-account fraud detection method -> break point a change in the behavior
(see slides) 
training part and testing part 
studying the difference -> find something that needs to be investigated
->peer-grouo
INTER-account techiques
-> association rule analysis 
association  between two different items in the dataset 



FRAUD DETECTION
in the previuour lecture we explodred the unsupervised, the main difference with rw supervised 
you don't have any label related to the custom value you want to predict/classify
we have to extract the norm
main challenge of unsupervised-> anomaly DETECTIONyou want to detect something that is outling with the norm itself
allows you to extract knowledge
started from graphic techniques-> something that outlies from the nrm, group of points
statistical: this time exploiting statistical techniques
-break point: intraaccount , you decide testing window and you test if the new distribution is similar to the preboous one
-peer-group, seasonality, data that change during time 
-


CLUSTERING: main unsupervised learning technique
process that requiresa a good amount of data, 
group data in different set 
norm: big cluster of data
anomalies: small cluster that is away from the normal 


Hierarchical and nonhierarchical
core element of clustering techiques: distance matric

in order to cluster element you have to compute the similarity between two elemnt 
-euclidea,/minkowski 

for continuois variables ok -> euclidian, pearson correlation 
categorical variables -> binary variables, zeo or one depending on if this particular features is present or not in your data

SMC computes number of identical matches between values
