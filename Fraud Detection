## 17/03/2022
Fraud detection: 

what is a fraud? 
criminal activity-> criminal deception intended to resuper in financial or personal gain. 
main driver of the froeds. If we use this definition to understand 

fraud is an uncommon, well-considered, imperceptibly concealed, time-evolving and often carefully organized crime. 
There 5 are the main challenges you have to cope with. 
It is not an isoleted phenomena, it is a social phenomenal, it comes to expenses of the victim. 
Who is the victim? organization, person, society. 

UNCOMMON: only a minority of cases concerns fraud, of which only a limited number owill be known to concern fraud. 
Only a minority of the cases in your datased concern fraud, only a small percentage of your datased is related to the class fraud. 
but, besides that, of this part, only a limited number will be known to concern fraud. Some of them may be not detected, but they are in your dataset. 
You will have to exploit as much as possible these few. 
-detect fraud is difficult: fraudulent cases are covered by the legitimate ones.
-learn from historical cases is difficult: since only few examples are available. 

WELL-CONSIDERED AND IMPERCEPTIBLY CONCEALED: the attacker tries to blend in the legitimate one. 
Fraudsters try to remain unnoticed and covered, blend in frauds, not behave different from non-fraudsters.
Fraudsters hide very well by well-considering and planning how to precisely commit fraud. Frauds are not impulsive and unplanned. 
mimic attacks, the attacker tries to mimic the behave of the user. 
frauds that evade from detection systems. 

TIME-EVOLVING: fraudsters adapt and refine their methods to remain undetected. Fraud detection systems improve and learn by example. 
The adversary is always one step further. Fraudsters techniques evolve in time aklong with or better ahed of fraud detection mechanisms. 

CAREFULLY ORGANIZED CRIME: every single fraud cases is not an isoleted event. 
Fraudsters: do not operate independently, involve complex and organized structures. To detect fraud the context should be taken into account. 


why do people commit fraud: 
Basic driver: potential monetary gain or benefit. 
-MID or GRiD: difficult financial situation, financial gain.
Fraud triangle: basic conceptual model that presents the factors that explain the drivers for an individual to ommit fraud. 
-Rationalisation, opportunity, motivation -> we have to reduce the opportunity driver. 

Fraud categories: 
Banking and credit card: one of the most done, economic motivation. Unauthorized taking of another's credit. 
1. create fake personal information and steal much money possible, steal from a financial istritution
2. obtain data of the bank account of an user and perform fraud

Insurance fraud: 
1.sell fake insurance
2. exploiit some condition to earn money

Counterfeit: imitate intended to be passed off fraudulently or deceprively as genuine-> concerns valuable objects. 

Product warranty fraud: fraudulently claiming compensation or remuneration based on a product warranty. 

Healthcare fraud
Telecommunications fraud: phishing, scamming, cloning, superimposition fraud. -> super easy to exploit, no phisical attack. Just the voice. 
Money laundering: criminal activity and make them appeal as legal. 
Click fraud: legal links on a website in order to add the number of clicks. 
Identity theft. 
Tax evasion. 
Plagiarism. 

Frauds impacts -> need to invest up to date defense infrastructures. 

### Anti-fraud strategy

Fraud prevention and fraud detection 
-Expert-based approach: 1. rule-based engine disadvantages -> expensive to build, require manual input, difficul to maintain and manage 
fraudsters can lerarn the rules and circumvent them, new frauyd patterns are not automatically updated
-automated fraud detection systems: how ever expert based remain important top build an effective 
first expert based -> then automated 

Fraud managment: all the activities that must take place after a fraudolent activity has been detected and confirmed 

-corrective measure: resolve and correct the consequencies 
-preventive: to prevent 


step futher-> automatated base -> frauds becomes earies to detect the more time has passed. 
the more time passed, the more a fraud mechanisms is used, if a fraud is done and it is not detected, it will be done again. 
the more data you have the mor eyou can build systems 

the more a fraud schema is used , the more easier to detect it is if time has passed
fraud paterns becomes more apparent, statistically easier to detect


DATA DRIVEM FRAUD DETECTION
like any other security mechanisms, there is a cost ofr operating 
if a label is correct and you r are reducing the loss, if the label is cnot correct: you have labeled as fraud a legit transaction
for every false positive you have a costsame for every false negative
but we cannot really investigate every transaction
in general we have a limited invastigation capacit

FRAUD DETECTION techniquesapprocahes are adopting powerful statostically-based methodologies and analyzing massive amounts of data
fraud remains hard to detect. 

Why it is challenging to detect fraud? 
fraudsters adapt theirs approaches to commit fraud without being exposed, probe fraud detection and prevention systsmes to understand their functioning to discover their weaknesses
Fraudsters develop advances strategies to cover/blend in their tracks to avoid being detected 



## 18/03/2022
### Fraud

unsupervised leanring techniques-> they do not require labeled observations
learn from historical observationallow detecting novel fraud patterns 

supervised look inside the huge amount of data 
unserivised: how they behave. 
extract features from data 
such kind of systems are not enough , complement the system with supervised.
learn the diffrence between the known 

un vs sup .> you want to apply unservised every time you ahve a data set but not labeled
supervaside data with label legitimed and unlegitimed -> predict, detect and estimate some features

limitation of supervised learning techiques: is it not enough 

start knownladge from the domain 

antifraud detection system I have to consider the information I have available


start with unsupervised learning that extract novel patterns, once you have the investigation of the anaylist you start with data from the past, find pattern and then supervised 

Fraud managment cycle: 
Fraud detection -> detect it 
Fraud Investigation: every thing flagged as fraud must be investigate by humane EVERY TIME, in order to check if it is a really fraud or not, it's important, allows to understand the knowledgne to use for automted systems
Fraud Confirmation: not, ok this is not fraudolent, change the label, retrain your system, unless put and coferm the label and go on 
Fraud Prevention : now we have to prevent these transaction

feedback loop-> you have to automatize all these process and we will automatize this process-> you have to update all your models and systems (the supervised and unsupervised to mathc the new pattern) 


Fraud analytics process model 



Fraud 3

Machine learning for Fraud Detection 
pefrorm detection 

Real data is typically dirtu, full of inconsistencies, incompleteness and duplication, from messy data you get messy analytical models
so we have to apply data filtering mechanisms
first step of data anlyting preprocessing part
data preprocessing step -> 80% of your time
lose a lot of time here, data are pain if you have ever worked with a real dataset

Data-filtering mechanisms must be applied to clean up and reduce the data.
Even the slightest mistake can make the data totally unusable and the results invalid.
 
-transactional data: structured and detailed information captyring the key characteristics of a customer transaction
Summarized over longer time horizons by aggregating it -> they are meaningful when interpreted individually and furthermore their interaction is useful for fraud detectio  and anti money laundering
-types of data elements
continuous data: defined on an interval ( limited and unlimited) 
categorial:all the set of data defined on a category  
  -nominal: no meaniningful ordering between a limited set of data
  -ordinal: meaningful ordering
  -binary: two values 

-sampling: take a subset of historical data to build an analytical model 
we not analyze directly the full data set because the key requirement for a good sample is to be representative fot the future entities 


 
Take a subset of historical data to build an analytical model.
With the availability of high-performance computing
why not analyze directly the full data set?
 
  sampling procedure: problem of bias, biased on your decision/selection
  taking a sample is biased by our decision
  a good sample should not be biased 
  model and select sample that target the avaerage period and behavior, i want an average picture
  but this is not an easy task 

what can I do ? 
-build separate models for homogenous time frames or different frames-> complex and demanding solution
-sampling observetions over a period covering a full business cycle and build a single model

Stratified sampling: a sample is taken according to predefined strata-> stratifying according to the target fraud indicator-> sample will contain exactly the same percentages of (non-) fraudulent transactions as in the original data
-> stratification applied on predictor variables-> resemble the real product transaction distribution 



Visual data exploration is important, gives you an initial insights into the data, you can use plots
Then inspect some basic statistical measurements -> calculate them separately for each of the target classes to see whether there are any interesting patterns present 

-basic descriptive statistics
descriptive statistics provide basic insight for the data, they should be assessed together in support and completion of each other
-MEAN AND MEDIAN
-VARIATION/STANDARD VARIATION 
-PERCENTILE VALUES
-MODE

-specific descritive statistics
-SYMMETRY OR ASYMMETRY OF A DISTRIBUTION -> harder to interpret: limits their pratical use, sometimes it is easier to assess these aspects by inspecting visual plots of the distribution of the involved variables

-missing values
Some analytical techniques (e.g., decision trees) can deal directly with missing values. Other techniques need some additional preprocessing.
 a. 
 1. Replace the missing values with a known value
 2. Delete observations or variables with lots of missing values: assumes information has no meaningful interpretation 
 3. Keep missing values since they can be meaningful and may have relation with fraud and nees to be considered as a separate category 
b. Statistically test whether missing information is related to the target variable or not.
1. if yes, then we can adopt the keep strategy and make a special category for it
2. if not, one can depending on the number of observations available- delete or replace

-outliers 
extreme observations that are very dissimilar to the rest of the population, can be valid or invalid , unvariate(one dimension) or multivariate(multiple dimensions)
  UNIVARIATE OUTLIER DETECTION AND TREATMENS
  .minimum and maximum values for each of the data elements 
  .graphical tools: histograms, box plot, z-scores
  compute the distribution, represent it on an istogram 
  if inside the range, plot all the data on this line, you compute statistics on your data, then you plot the rane first/last quartile-> how much of your data is inside this area
  ho wmany stanrdard deviation an observation lies away from the mean-> you check for each single instance how much differs from the media in terms of standard deviation 

  MULTIVARIATE OUTLIER DETECTION AND TREATMENS
  -distance between different vectors
  .fitting regression lines and inspecting the observations with large errors
  .clustering or calculating the Mahalanobis distance


-variouys schemes exist to deal with outliers 
  for invalid observations one could treat the putlier as a missing value
  for valid obs impose both a lower and upper limit on a variable and any values below/above are brought back to these limits 

Construct a set of rules formulated based on expert knowledge 
find relation you know must be present in your dataset
constraint that apply 

take explicit precautions

theser kind of inconsistences can by find by experience
## preprocessing
extremly careful to treat these values, you have to ducment by addign a feature that specify because 
it is missing a valueif you start a preprocessing in the wrong way you cannot be more able to detect the anomaly 

another problem: represented by this image : INSERT image
compare two value of your dataset -> the distance ( eucledian, manhattan..)
the problem of this computation is that it is not correct ? 
The scale are differt!!! doesn't make any sense 
u have to standardize your features !!
standardize data-> scaling variables to a similar range
-mix/man standardize
-z-score 
-decimal scaling 

## Categorization 
Additional information about the domain, sometimes you want to add to the dataset some knoldegne of the domain you are analyzing 
some times you have also continuous variables, and uoi have to study the slot and defining when there is a change (increase, dcrease, ioncrease) 
depending on the data you may want to extract different informatio that can be useful -> but you are adding complexity, feeatures to te original one that must be treated

## Variable selection 
problem automatically solved by random forest alghoritms, they learn the feature important to you, but in another case you don't want to spend complexity and add features to your model 
this is a problem cause sometimes the dataset has a huge amount of features, and in the end only few of them are really helpful-> 10/15 variables

depending on the type of the variable and the type of target variable  you can select a different statistic test that measure the correlation between the target and the variable

additional metodology to analyze the correlation


sometimes you cannot use all the feature 
the motivation may contain personal information 
feature seletion independent from your decision 

## PCA 
group of methods to automatically reduce the dimensionality of your dataset and at the same time finds a set of feature not correlated with each other
you are able to reduce, select feature and the feature are not correlated

reduce dim by forming new value that are a linear composition of the original ones (main component) 
at maximum the same number of the original ones 
only a small portion of your variable contains the information you need 

find two new features that better describe the distribution of the data in the space
 PC1 will describe the set of data in a way that you can continue the analysis

 ## correlation and stability 
 assure that your model will be stable


trade-off: PCA if your are not interested in interpretaibility 
if you need to have a score easy to investigate/interpret, you have to avoid PCA


# DESCRIPTIVE ANALYTICS FOR FRAUD DETECTION
unservised learning techniques
unsupervides aims at finding anomalos behavior that deviates from the norm of the dataset
you have to define the norm ( average behiuour of dataset of customer)

unsupervised , you can identify from know cyber attack 

techiques relevant when you have NO labels

define your norm, define the AVERAGE behavior of your dataset 

procedure similar for outliers in preprocessing, you can canalyze them in one or multi dimension 


knowledge to verify what you have found

statistical outlier detection 
-> z-score
-fit a distribution 
these two are the same saw before
-> break point analysis
instra-account fraud detection method -> break point a change in the behavior
(see slides) 
training part and testing part 
studying the difference -> find something that needs to be investigated
->peer-grouo
INTER-account techiques
-> association rule analysis 
association  between two different items in the dataset 



FRAUD DETECTION
in the previuour lecture we explodred the unsupervised, the main difference with rw supervised 
you don't have any label related to the custom value you want to predict/classify
we have to extract the norm
main challenge of unsupervised-> anomaly DETECTIONyou want to detect something that is outling with the norm itself
allows you to extract knowledge
started from graphic techniques-> something that outlies from the nrm, group of points
statistical: this time exploiting statistical techniques
-break point: intraaccount , you decide testing window and you test if the new distribution is similar to the preboous one
-peer-group, seasonality, data that change during time 
-


CLUSTERING: main unsupervised learning technique
process that requiresa a good amount of data, 
group data in different set 
norm: big cluster of data
anomalies: small cluster that is away from the normal 


Hierarchical and nonhierarchical
core element of clustering techiques: distance matric

in order to cluster element you have to compute the similarity between two elemnt 
-euclidea,/minkowski 

for continuois variables ok -> euclidian, pearson correlation 
categorical variables -> binary variables, zeo or one depending on if this particular features is present or not in your data

SMC computes number of identical matches between values





HIERARCHICAL CLUSTERING 
NON HIERARCHICAL CLUSTERING
vedi slide/Tancredi 


the final result of clustering technique is a label dataset
use the output and train a supervised learning one with the lbel of the cluster techniques
